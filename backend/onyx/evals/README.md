# Onyx Evaluations

This directory contains the evaluation framework for testing and measuring the performance of Onyx's chat and retrieval systems.

## Overview

The evaluation system uses [Braintrust](https://www.braintrust.dev/) to run automated evaluations against test datasets. It measures the quality of responses generated by Onyx's chat system and can be used to track performance improvements over time.

## Prerequisites

**Important**: The model server must be running in order for evals to work properly. Make sure your model server is up and running before executing any evaluations.

## Running Evaluations

Kick off a remote job
```bash
onyx/backend$ python -m dotenv -f .vscode/.env run -- python onyx/evals/eval_cli.py --remote --api-key <SUPER_CLOUD_USER_API_KEY> --search-permissions-email <email account to reference> --remote --remote-dataset-name Simple
```

You can also run the CLI directly from the command line:

```bash
onyx$ python -m dotenv -f .vscode/.env run -- python backend/onyx/evals/eval_cli.py --local-dataset-path backend/onyx/evals/data/eval.json --search-permissions-email richard@onyx.app
```
make sure your AUTH_TYPE=disabled when running evals locally. Save the env var ONYX_EVAL_API_KEY in your .env file so you don't 
have to specify it every time for triggering remote runs. 


### Production Environment

### Local Development

For local development, use the `eval_cli.py` script. We recommend starting it from the VS Code launch configuration for the best debugging experience.

#### Using VS Code Launch Configuration

1. Open VS Code in the project root
2. Go to the "Run and Debug" panel (Ctrl/Cmd + Shift + D)
3. Select "Eval CLI" from the dropdown
4. Click the play button or press F5

This will run the evaluation with the following default settings:
- Uses the local data file at `evals/data/data.json`
- Enables verbose output
- Sets up proper environment variables and Python path

#### CLI Options

- `--local-data-path`: Path to local JSON file containing test data (defaults to `evals/data/data.json`)
- `--remote-dataset-name`: Name of remote Braintrust dataset
- `--braintrust-project`: Braintrust project name (overrides `BRAINTRUST_PROJECT` env var)
- `--verbose`: Enable verbose output
- `--no-send-logs`: Skip sending logs to Braintrust (useful for local testing)
- `--local-only`: Run evals locally without Braintrust, output results to CLI only

## Test Data

The evaluation system uses test data stored in `evals/data/data.json`. This file contains a list of test cases, each with:
- `input`: The question or prompt to test

Example test case:
```json
{
    "input": {
      "message": "What is the capital of France?"
    }
}
```

### Per-Test Configuration

Configure tool forcing, assertions, and model settings per-test by adding optional fields to each test case.

#### Tool Configuration

- `force_tools`: List of tool type names to force for this specific test
- `expected_tools`: List of tool type names expected to be called
- `require_all_tools`: If true, all expected tools must be called (default: false)

#### Model Configuration

- `model`: Model version to use (e.g., "gpt-4o", "claude-3-5-sonnet")
- `model_provider`: Model provider (e.g., "openai", "anthropic")
- `temperature`: Temperature for the model (default: 0.0)

Example with tool and model configuration:
```json
[
  {
    "input": {
      "message": "Find information about Python programming"
    },
    "expected_tools": ["SearchTool"],
    "force_tools": ["SearchTool"],
    "model": "gpt-4o"
  },
  {
    "input": {
      "message": "Search the web for recent news about AI"
    },
    "expected_tools": ["WebSearchTool"],
    "model": "claude-3-5-sonnet",
    "model_provider": "anthropic"
  },
  {
    "input": {
      "message": "Calculate 2 + 2"
    },
    "expected_tools": ["PythonTool"],
    "temperature": 0.5
  }
]
```

### Multi-Turn Evaluations

For testing realistic multi-turn conversations where each turn may require different tools, use the `messages` array format instead of a single `message`:

```json
{
  "input": {
    "messages": [
      {
        "message": "What's the latest news about OpenAI today?",
        "expected_tools": ["WebSearchTool", "OpenURLTool"]
      },
      {
        "message": "Now search our internal docs for our OpenAI integration guide",
        "expected_tools": ["SearchTool"]
      },
      {
        "message": "Thanks, that's helpful!",
        "expected_tools": []
      }
    ]
  }
}
```

Each message in the `messages` array can have its own configuration:
- `message`: The user message text (required)
- `expected_tools`: List of tool types expected to be called for this turn
- `require_all_tools`: If true, all expected tools must be called (default: false)
- `force_tools`: List of tool types to force for this turn
- `model`: Model version override for this turn
- `model_provider`: Model provider override for this turn
- `temperature`: Temperature override for this turn

Multi-turn evals run within a single chat session, so the model has full context of previous turns when responding.

### Available Tool Types

The following built-in tool types can be used:
- `SearchTool`: Internal document search
- `WebSearchTool`: Internet/web search
- `ImageGenerationTool`: Image generation
- `PythonTool`: Python code execution
- `OpenURLTool`: Open and read URLs

### Braintrust Dashboard

After running evaluations, you can view results in the Braintrust dashboard. The evaluation will report:
- `tool_assertion`: Score of 1.0 if tool assertions passed (or no assertions configured), 0.0 if failed
- Metadata including `tools_called`, `tools_called_count`, and assertion details
